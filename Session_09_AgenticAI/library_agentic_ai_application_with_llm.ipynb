{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95de6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierAgent Initial state: {'question': 'When does library open?'}\n",
      "ChatCompletion(id='chatcmpl-CSQ7FAOLGOeEbVnqIiMu5rxxHVY4G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"faq_answer\": \"Library hours vary by branch. Please tell me which branch you’re asking about, or check the Hours page on our website for the most up-to-date opening times. If you’d like, I can look up the hours for a specific branch.\",\\n  \"checkout_info\": null\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760889609, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=777, prompt_tokens=59, total_tokens=836, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=704, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Raw response: {\n",
      "  \"faq_answer\": \"Library hours vary by branch. Please tell me which branch you’re asking about, or check the Hours page on our website for the most up-to-date opening times. If you’d like, I can look up the hours for a specific branch.\",\n",
      "  \"checkout_info\": null\n",
      "}\n",
      "Parsed response: {'faq_answer': 'Library hours vary by branch. Please tell me which branch you’re asking about, or check the Hours page on our website for the most up-to-date opening times. If you’d like, I can look up the hours for a specific branch.', 'checkout_info': None}\n",
      "CheckoutAgent Initial state: {'question': 'When does library open?', 'faq_answer': 'Library hours vary by branch. Please tell me which branch you’re asking about, or check the Hours page on our website for the most up-to-date opening times. If you’d like, I can look up the hours for a specific branch.', 'checkout_info': None}\n",
      "FAQAgent Initial state: {'question': 'When does library open?', 'faq_answer': 'Library hours vary by branch. Please tell me which branch you’re asking about, or check the Hours page on our website for the most up-to-date opening times. If you’d like, I can look up the hours for a specific branch.', 'checkout_info': None}\n",
      "CheckouResponseAgent Initial state: {'question': 'When does library open?', 'faq_answer': 'Library hours vary by branch. Please tell me which branch you’re asking about, or check the Hours page on our website for the most up-to-date opening times. If you’d like, I can look up the hours for a specific branch.', 'checkout_info': 'Checkout info: Not requested'}\n",
      "ChatCompletion(id='chatcmpl-CSQ7KYKzmk7zzNH1dSCzUfoIfFkOp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Library hours vary by branch. Please specify which branch you're interested in, or you can check the Hours page on our website for the most up-to-date opening times. If you'd like, I can look up the hours for a specific branch for you.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760889614, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_1f35c1788c', usage=CompletionUsage(completion_tokens=50, prompt_tokens=107, total_tokens=157, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Final Answer ---\n",
      "Library hours vary by branch. Please specify which branch you're interested in, or you can check the Hours page on our website for the most up-to-date opening times. If you'd like, I can look up the hours for a specific branch for you.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print (f'Key is {my_api_key}')\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=my_api_key)\n",
    "\n",
    "\n",
    "# --- Shared State ---\n",
    "class LibraryState(TypedDict):\n",
    "    question: Optional[str]\n",
    "    faq_answer: Optional[str]\n",
    "    checkout_info: Optional[str]\n",
    "    final_answer: Optional[str]\n",
    "\n",
    "\n",
    "def ClassifierAgent(state: LibraryState):\n",
    "    \n",
    "    print(f\"ClassifierAgent Initial state: {state}\")\n",
    "\n",
    "    # Build the LLM messages\n",
    "    message_to_llm = [\n",
    "        {\"role\": \"system\", \"content\": '''You are a classifier agent in a library system. \n",
    "        Decide if the user is asking about book availability/checkout or about library FAQs. \n",
    "        Reply with JSON containing keys: faq_answer and checkout_info.'''},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {state['question']}\"}\n",
    "    ]\n",
    "    # Call the OpenAI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=message_to_llm,\n",
    "        # temperature=0.2,   # keep it deterministic for classification\n",
    "        # max_tokens=150,\n",
    "    )\n",
    "    print (response)\n",
    "    # Extract the content from the response\n",
    "    answer = response.choices[0].message.content\n",
    "    # Ideally, parse as JSON — here assuming model returns a dict-like string\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(answer)\n",
    "        print(f\"Raw response: {answer}\")\n",
    "        print(f\"Parsed response: {parsed}\")\n",
    "\n",
    "        return {\n",
    "            \"faq_answer\": parsed.get(\"faq_answer\", \"\"),\n",
    "            \"checkout_info\": parsed.get(\"checkout_info\", \"\")\n",
    "        }\n",
    "    except Exception:\n",
    "        # fallback if LLM gives plain text\n",
    "        return {\"faq_answer\": answer, \"checkout_info\": \"\"}\n",
    "\n",
    "def FAQAgent(state: LibraryState):\n",
    "    \n",
    "    print(f\"FAQAgent Initial state: {state}\")\n",
    "    if not state.get(\"faq_answer\"):\n",
    "        return {\"faq_answer\": \"Default FAQ: Library rules apply\"}\n",
    "    return {\"faq_answer\": state[\"faq_answer\"]}\n",
    "\n",
    "\n",
    "def CheckoutAgent(state: LibraryState):\n",
    "    print(f\"CheckoutAgent Initial state: {state}\")\n",
    "    if not state.get(\"checkout_info\"):\n",
    "        return {\"checkout_info\": \"Checkout info: Not requested\"}\n",
    "    return { \"checkout_info\": state[\"checkout_info\"]}\n",
    "\n",
    "\n",
    "def ResponseAgent(state: LibraryState):\n",
    "    print(f\"CheckouResponseAgent Initial state: {state}\")\n",
    "  \n",
    "    message_to_llm = [\n",
    "        {\"role\": \"system\", \"content\": '''You are a response builder for the library application.\n",
    "          Please combine the FAQ answer and checkout info into a coherent response to the user's question.\n",
    "        '''},\n",
    "        {\"role\": \"user\", \"content\": f\"FAQ: {state['faq_answer']}\\nCheckout Info: {state['checkout_info']}\\nQuestion: {state['question']}\"}\n",
    "    ]\n",
    "\n",
    "    # Call the OpenAI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=message_to_llm,\n",
    "        # temperature=0.2,   # keep it deterministic for classification\n",
    "        # max_tokens=150,\n",
    "    )\n",
    "\n",
    "    print (response)\n",
    "    # Extract the content from the response\n",
    "    final_answer = response.choices[0].message.content\n",
    "\n",
    "\n",
    "    return {\"final_answer\": final_answer}\n",
    "\n",
    "\n",
    "# --- Build the Graph ---\n",
    "builder = StateGraph(LibraryState)\n",
    "builder.add_node(\"ClassifierAgent\", ClassifierAgent)\n",
    "builder.add_node(\"FAQAgent\", FAQAgent)\n",
    "builder.add_node(\"CheckoutAgent\", CheckoutAgent)\n",
    "builder.add_node(\"ResponseAgent\", ResponseAgent)\n",
    "\n",
    "builder.add_edge(START, \"ClassifierAgent\")\n",
    "builder.add_edge(\"ClassifierAgent\", \"FAQAgent\")\n",
    "builder.add_edge(\"ClassifierAgent\", \"CheckoutAgent\")\n",
    "builder.add_edge(\"FAQAgent\", \"ResponseAgent\")\n",
    "builder.add_edge(\"CheckoutAgent\", \"ResponseAgent\")\n",
    "builder.add_edge(\"ResponseAgent\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = graph.invoke({\"question\": \"When does library open?\"})\n",
    "print(\"\\n--- Final Answer ---\")\n",
    "print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45969394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # --- Visualize ---\n",
    "# print(graph.get_graph().draw_ascii())\n",
    "# graph.get_graph().draw_png(\"images/agentic_ai_library.png\")\n",
    "# print(\"Graph saved as agentic_ai_library.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d8aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierAgent ran\n",
      "state: {'question': 'When does library open?'}\n",
      "ChatCompletion(id='chatcmpl-CDDw5hU1p1SMaV3rapBjn6VpnU4p5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"faq_answer\": \"The library opens at 9:00 AM and closes at 5:00 PM from Monday to Saturday. It is closed on Sundays.\", \"checkout_info\": null}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757267869, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7c233bf9d1', usage=CompletionUsage(completion_tokens=39, prompt_tokens=60, total_tokens=99, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Raw response: {\"faq_answer\": \"The library opens at 9:00 AM and closes at 5:00 PM from Monday to Saturday. It is closed on Sundays.\", \"checkout_info\": null}\n",
      "Parsed response: {'faq_answer': 'The library opens at 9:00 AM and closes at 5:00 PM from Monday to Saturday. It is closed on Sundays.', 'checkout_info': None}\n",
      "CheckoutAgent ran\n",
      "FAQAgent ran\n",
      "FAQAgent state {'question': 'When does library open?', 'faq_answer': 'The library opens at 9:00 AM and closes at 5:00 PM from Monday to Saturday. It is closed on Sundays.', 'checkout_info': None}\n",
      "ResponseAgent ran\n",
      "ChatCompletion(id='chatcmpl-CDDw6j8UB1p0krGxL3tugW2ZsEp48', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"At nine o'clock each Monday morn,  \\nThe library opens, welcoming, warm.  \\nTuesday through Saturday, same time here,  \\nFive pm closes, then it’s time to clear.  \\n\\nOn Sundays, silence holds the door,  \\nClosed, the library waits once more.  \\nNo checkout info requested, true,  \\nJust know these hours are clear for you.  \\n\\nSo plan your visit from nine to five,  \\nAnd read or study, thrive, and thrive.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757267870, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7c233bf9d1', usage=CompletionUsage(completion_tokens=95, prompt_tokens=90, total_tokens=185, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Final Answer ---\n",
      "At nine o'clock each Monday morn,  \n",
      "The library opens, welcoming, warm.  \n",
      "Tuesday through Saturday, same time here,  \n",
      "Five pm closes, then it’s time to clear.  \n",
      "\n",
      "On Sundays, silence holds the door,  \n",
      "Closed, the library waits once more.  \n",
      "No checkout info requested, true,  \n",
      "Just know these hours are clear for you.  \n",
      "\n",
      "So plan your visit from nine to five,  \n",
      "And read or study, thrive, and thrive.\n"
     ]
    }
   ],
   "source": [
    "# --- Run ---\n",
    "result = graph.invoke({\"question\": \"When does library open?\"})\n",
    "print(\"\\n--- Final Answer ---\")\n",
    "print(result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b69c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierAgent ran\n",
      "state: {'question': 'Is The Hobbit available?'}\n",
      "ChatCompletion(id='chatcmpl-CDDwDyrvTkY8mrGyjeHj5bph25qOZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"faq_answer\": null,\\n  \"checkout_info\": \"The Hobbit is available for checkout. Please visit the library or check online to reserve a copy.\"\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757267877, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_04d3664870', usage=CompletionUsage(completion_tokens=34, prompt_tokens=60, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Raw response: {\n",
      "  \"faq_answer\": null,\n",
      "  \"checkout_info\": \"The Hobbit is available for checkout. Please visit the library or check online to reserve a copy.\"\n",
      "}\n",
      "Parsed response: {'faq_answer': None, 'checkout_info': 'The Hobbit is available for checkout. Please visit the library or check online to reserve a copy.'}\n",
      "CheckoutAgent ran\n",
      "FAQAgent ran\n",
      "FAQAgent state {'question': 'Is The Hobbit available?', 'faq_answer': None, 'checkout_info': 'The Hobbit is available for checkout. Please visit the library or check online to reserve a copy.'}\n",
      "ResponseAgent ran\n",
      "ChatCompletion(id='chatcmpl-CDDwEKB0N5f1IGycvb2lWOFNAWs4P', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Yes, The Hobbit can be yours today,  \\nIt’s available for checkout without delay.  \\nVisit the library or go online with ease,  \\nReserve your copy, then settle in to read the stories that please.  \\n  \\nRemember, library rules apply—  \\nRespect the lending process, don't let good stories pass by.  \\nSo, come and borrow with joy and cheer,  \\nYour adventure in Middle-earth awaits right here!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757267878, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7c233bf9d1', usage=CompletionUsage(completion_tokens=84, prompt_tokens=82, total_tokens=166, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Final Answer ---\n",
      "Yes, The Hobbit can be yours today,  \n",
      "It’s available for checkout without delay.  \n",
      "Visit the library or go online with ease,  \n",
      "Reserve your copy, then settle in to read the stories that please.  \n",
      "  \n",
      "Remember, library rules apply—  \n",
      "Respect the lending process, don't let good stories pass by.  \n",
      "So, come and borrow with joy and cheer,  \n",
      "Your adventure in Middle-earth awaits right here!\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"Is The Hobbit available?\"})\n",
    "print(\"\\n--- Final Answer ---\")\n",
    "print(result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa647a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
